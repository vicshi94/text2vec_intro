---
title: "test1"
author: "vicshi94"
date: "2024-02-23"
output: html_document
---

# text2vec in R

这是一个 *text2vec* 库的介绍文档。text2vec version == 0.6.4. 所以有些代码会与其他文档不一致。

更多详情请参阅源码：<https://cran.r-project.org/web/packages/text2vec/vignettes/text-vectorization.html>

```{r load_packages}
# 读取库
# suppressMessages()用来不显示提示信息, 同理还有suppressWarnings()函数
suppressMessages(library(text2vec))
```

## 1. 相似性研究（TCM+GloVe）

数据地址：<https://github.com/m-clark/text-analysis-with-R/blob/master/data/text8.zip>

```{r load_wiki_data}
wiki = readLines("text8", n = 1, warn = FALSE)
```

### 1.1 创建一个迭代器

让create_vocabulary, create_dtm, vectorizers, create_tcm等函数能够以内存友好的方式（分批次处理）处理大型语料库

```{r itoken_intro}
it = itoken(
  wiki, # 语料库
  preprocessor = tolower, # 小写
  tokenizer = space_tokenizer, # 注释1
  # n_chunks = 10, 将语料库分成10块处理
  # ids = , 文档id，如果没有指定，将自动创建
  progressbar = FALSE # 不显示进度条
)
```

\*注释1: tokenizer常用的是word_tokenizer, 此处使用space_tokenizer是为了加速处理。space_tokenizer的原理是严格按空格切分单词，所以处理速度更快，但处理效果不如word_tokenizer：

```{r compare_tokenizer, results='hold'}
test_sentence <- "word1 word2. word3?"
word_tokenizer_res <- word_tokenizer(test_sentence)[[1]]
space_tokenizer_res <- space_tokenizer(test_sentence)[[1]]
print(paste("原句：",test_sentence))
print(paste("word_tokenizer：", paste(word_tokenizer_res,collapse = " ")))
print(paste("space_tokenizer：", paste(space_tokenizer_res,collapse = " ")))
```

### 1.2 创建词汇表

```{r create_vocabulary}
vocab = create_vocabulary(it)
```

当然，我们可以使用N-grams优化我们词汇表

```{r create_vocabulary_ngram, results='hold'}
vocab_2grams = create_vocabulary(
  it, ngram = c(1L, 2L) # ngram_min = 1, ngram_max = 2
)

print(paste("原来词汇表长度：", dim(vocab)[1]))
print(paste("2-grams的词汇表长度：", dim(vocab_2grams)[1]))
```

词汇表长度显著提升！这样不仅会导致后续模型训练时间变得很长，同时也会影响模型准确率。因此我们需要对词汇表进行剪枝操作（还是以原词汇表为例）

### 1.3 词汇表修剪

第一种方法是添加停用词

```{r stop_words, eval=FALSE}
# 第一种方法是添加一些stopwords
stop_words = c("i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours")
vocab = create_vocabulary(
  it,
  stopwords = stop_words
  )
```

第二种方法使用text2vec提供的剪枝函数

```{r prune_vocabulary, results='hold'}
pruned_vocab = prune_vocabulary(
  vocab,
  term_count_min = 10, # words最小出现频次
  # doc_proportion_max = 0.5, # words出现在多少的文档中
  # term_count_max, doc_proportion_min, doc_count_min, doc_count_max,
  # vocab_term_max -> maximum number of terms in vocabulary
  ) 

print(paste("原来词汇表长度：", dim(vocab)[1]))
print(paste("修剪后词汇表长度：", dim(pruned_vocab)[1]))

# 创建一个后续可用于各种文本分析所需的矩阵分解的单词数据结构
vectorizer = vocab_vectorizer(vocab)
```

### 1.4 映射构建（1）：词共现矩阵

Term-co-occurrence Matrix，TCM. 常常用于GloVe模型

示例图：

![](images/clipboard-1100734808.png)

```{r tcm}
tcm = create_tcm(
  it, vectorizer, 
  skip_grams_window = 5L, # 共现窗口大小
  # skip_grams_window_context = "symmetric" OR "right" OR "left"
  )
```

### 1.5 GloVe

```{r glove, results='hide'}
# 创建一个glove对象
glove = GlobalVectors$new(
  rank = 50, # 预期词向量维度
  # 此处50仅为计算方便，实际使用时建议300或参考文献
  x_max = 10, # 加权函数中最大共现词数量
  # 关于glove参数，请参阅：http://nlp.stanford.edu/pubs/glove.pdf
  )

wv_main = glove$fit_transform(
  tcm, 
  n_iter = 25, # 训练迭代次数
  convergence_tol = 0.001,
  # n_threads = 8
  ) # 为了加速运算，这里都用了不太严格的设置**

# 获取词向量
wv_context = glove$components
word_vectors = wv_main + t(wv_context)
```

\*\*注释2: glove\$fit_transform() 中很多参数涉及到机器学习知识，在此不作延伸。涉及术语包括但不限于随机梯度下降（SGD）、迭代方法、收敛（convergence）、学习率（learning rate）。

PS: 在找资料的过程中看到不建议设置learning rate，因为使用了AdaGrad算法，但本人没有进一步拆包核实。

### 1.6 示例：词相似度

![](images/clipboard-3095517217.png)

下面我们将检验两组词向量关系：

paris - france + germany = ?

son - {man words} + {woman words} = ?

```{r berlin_test}
test1 = word_vectors["paris", , drop = FALSE] -
  word_vectors["france", , drop = FALSE] +
  word_vectors["germany", , drop = FALSE]
# 相似度
test1_cos_sim = sim2(
  x = word_vectors, y = test1, 
  method = "cosine", #计算方法：余弦相似度
  norm = "l2") # L2正则化是指向量中各个元素的平方和然后再求平方根
# 我们的模型比较粗糙，因此放宽到看看前五个单词中有没有“berlin”
head(sort(test1_cos_sim[,1], decreasing = TRUE), 5)
```

```{r son_test}
test2 = word_vectors["son", , drop = FALSE] -
  word_vectors["man", , drop = FALSE] +
  word_vectors["woman", , drop = FALSE] -
  word_vectors["men", , drop = FALSE] +
  word_vectors["women", , drop = FALSE] #***

test2_cos_sim = sim2(x = word_vectors, y = test2, method = "cosine", norm = "l2")
head(sort(test2_cos_sim[,1], decreasing = TRUE), 5)
```

\*注释3: 可以通过增加关系对来强化某一个维度，很多文献中有提供详细的group words，如Garg et al. (2018).

Garg, N., Schiebinger, L., Jurafsky, D., & Zou, J. (2018). Word embeddings quantify 100 years of gender and ethnic stereotypes. *Proceedings of the National Academy of Sciences*, *115*(16). <https://doi.org/10.1073/pnas.1720347115>

## 2. 下游任务，以情感分析为例

上一部分我们主要停留在word embedding部分，然而当将text转化为data在很多时候仅是开始，它可以进一步服务于众多下游任务，如主题模型、监督学习任务。下一部分我们将以sentiment analysis (supervised learning) 为例继续介绍text2vec.

### 2.1 Sampling

```{r movie_review}
suppressMessages(library(data.table))

# movie_review 是text2vec自带的案例数据
data("movie_review") 
setDT(movie_review) # 将数据转换为data.table格式
setkey(movie_review, id) # 设置主键

# head(movie_review)
```

![](images/clipboard-1730925313.png)

下面我们将数据分为训练集和测试集进行监督学习任务

```{r sampling}
set.seed(20240223) # 设置随机seed以便结果可重复
train_idx <- sample(nrow(movie_review), 0.8 * nrow(movie_review)) # 生成训练集索引
train_set <- movie_review[train_idx] # 生成训练集
test_set <- movie_review[-train_idx] # 生成测试集
# print(head(train_set)) # 查看训练集前几行
```

```{r it_samples}
it_train = itoken(
  train_set$review, # 语料
  preprocessor = tolower, 
  tokenizer = word_tokenizer, 
  ids = train_set$id,
  progressbar = FALSE
  )

vocab_train = create_vocabulary(it_train)
vectorizer_train = vocab_vectorizer(vocab_train)

it_test = itoken(
  test_set$review, # 语料
  preprocessor = tolower, 
  tokenizer = word_tokenizer, 
  ids = test_set$id,
  progressbar = FALSE
  )

vocab_test = create_vocabulary(it_test)
vectorizer_test = vocab_vectorizer(vocab_test)
```

### 2.2 映射构建（2）: DTM

上一部分我们重点介绍了TCM，这一部分我们将介绍另一种映射构建方式，document-term matrix (DTM).

```{r dtm, warning=FALSE}
# 生成一个基于词汇表的DTM
# Warning：下面的变量名已更换，请勿调用成Sec1中的it，vectorizer
dtm_train = create_dtm(
  it_train,
  vectorizer_train
  )
print(dim(as.matrix(dtm_train))) # 查看DTM的维度
```

### 2.3 DTM优化

第一种方法：标准化

将 DTM 到将不同长度到文档调整到一个共同的尺度上。针对不同长度的文档，我们使用 L1 标准化，及将每一列的数值转换为和为 1，这个转换可以改进我们数据预处理的质量。

第二种方法：TF-IDF

### 2.4 拟合 & AUC

------------------------------------------------------------------------

未整合部分代码：

```{r}
## 6. 利用TF-IDF优化DTM --------------------------------
vocab = create_vocabulary(it_train)
vectorizer = vocab_vectorizer(vocab)
dtm_train = create_dtm(it_train, vectorizer)
dtm_test = create_dtm(it_test, vectorizer)

tfidf = TfIdf$new() # 创建一个TF-IDF对象
dtm_train_tfidf = fit_transform(dtm_train, tfidf) # 训练集TF-IDF
dtm_test_tfidf = transform(dtm_test, tfidf) # 测试集TF-IDF

res = cv.glmnet(x = dtm_train_tfidf, y = train_set[['sentiment']], 
                family = 'binomial', 
                alpha = 1,
                type.measure = "auc",
                nfolds = NFOLDS,
                thresh = 1e-3,
                maxit = 1e3)
print(paste("max AUC =", round(max(res$cvm), 4)))

## 7. 词嵌入 --------------------------------
suppressMessages(library(magrittr))
data("movie_review")
it = itoken(
  movie_review$review, 
  tolower, 
  word_tokenizer
  )
v = create_vocabulary(it) %>% prune_vocabulary(term_count_min=10)
vectorizer = vocab_vectorizer(v)
tcm = create_tcm(
  it, vectorizer, 
  grow_dtm = FALSE, # 生成TCM过程中不生成DTM
  skip_grams_window = 5L) # window size of context
print(dim(tcm))

# GloVe
glove = GlobalVectors$new(rank = 50, x_max = 10)
wv_main = glove$fit_transform(
  tcm, 
  n_iter = 25, 
  convergence_tol = 0.01, 
  n_threads = 8
  )
wv_context = glove$components
word_vectors = wv_main + t(wv_context)

#Pass: w=word, d=dist matrix, n=nomber of close words
findCloseWords = function(w,d,n) {
  words = rownames(d)
  i = which(words==w)
  if (length(i) > 0) {
    res = sort(d[i,])
    print(as.matrix(res[2:(n+1)]))
  } 
  else {
    print("Word not in corpus.")
  }
}

#Make distance matrix
d = dist2(word_vectors, method="cosine")  #Smaller values means closer
print(dim(d))

findCloseWords("woman",d,10)
```

### 3. Task

这一部分仅会用到Sec1中的代码

```{r}
load('data/shakes_words_df_4text2vec.RData')
shakes_words_ls = list(shakes_words$word)
it = itoken(shakes_words_ls, progressbar = FALSE)
shakes_vocab = create_vocabulary(it)
shakes_vocab = prune_vocabulary(shakes_vocab, term_count_min = 5)
# maps words to indices
vectorizer = vocab_vectorizer(shakes_vocab)

# use window of 10 for context words
shakes_tcm = create_tcm(it, vectorizer, skip_grams_window = 10)
glove = GlobalVectors$new(word_vectors_size = 50, vocabulary = shakes_vocab, x_max = 10)
shakes_wv_main = glove$fit_transform(shakes_tcm, n_iter = 1000, convergence_tol = 0.00001)

# dim(shakes_wv_main)

shakes_wv_context = glove$components

# dim(shakes_wv_context)

# Either word-vectors matrices could work, but the developers of the technique
# suggest the sum/mean may work better
shakes_word_vectors = shakes_wv_main + t(shakes_wv_context)
rom = shakes_word_vectors["romeo", , drop = F]
# ham = shakes_word_vectors["hamlet", , drop = F]

cos_sim_rom = sim2(x = shakes_word_vectors, y = rom, method = "cosine", norm = "l2")
# head(sort(cos_sim_rom[,1], decreasing = T), 10)
love = shakes_word_vectors["love", , drop = F]

cos_sim_rom = sim2(x = shakes_word_vectors, y = love, method = "cosine", norm = "l2")
# head(sort(cos_sim_rom[,1], decreasing = T), 10)
```

1.  Romeo最相近的词
2.  Romen - {he} + {she}
